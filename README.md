Model: # ğŸ§  Multivariate Convolutional Regression

<div align="center">

**A powerful Convolutional Neural Network library for Multivariate Regression
with Incremental Online Learning**

[ğŸ“¦ JSR Package](https://jsr.io/@hviana/multivariate-convolutional-regression) â€¢
[ğŸ™ GitHub](https://github.com/hviana/polynomial-regression) â€¢
[ğŸ‘¤ Author: Henrique Emanoel Viana](#-author)

</div>

---

## ğŸ“‘ Table of Contents

- [âœ¨ Features](#-features)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [âš™ï¸ Configuration Parameters](#ï¸-configuration-parameters)
- [ğŸ“– API Reference](#-api-reference)
- [ğŸ¯ Optimization Guide](#-optimization-guide)
- [ğŸ’¡ Use Case Examples](#-use-case-examples)
- [ğŸ”§ Advanced Topics](#-advanced-topics)
- [ğŸ“„ License](#-license)

---

## âœ¨ Features

<div align="center">

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ğŸŒŸ CORE CAPABILITIES ğŸŒŸ                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”„ Online Learning      â”‚  ğŸ“Š Multi-Output       â”‚  ğŸ¯ Uncertainty Est.    â”‚
â”‚  Stream data in real-    â”‚  Handle multiple       â”‚  95% confidence         â”‚
â”‚  time, no batching       â”‚  target variables      â”‚  intervals included     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ›¡ï¸ Outlier Detection    â”‚  ğŸ“ˆ Drift Detection    â”‚  ğŸ’¾ Full Serialization  â”‚
â”‚  Auto-downweight         â”‚  ADWIN algorithm       â”‚  Save/load complete     â”‚
â”‚  anomalous samples       â”‚  detects changes       â”‚  model state            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âš¡ Adam Optimizer       â”‚  ğŸ“‰ Learning Schedule  â”‚  ğŸ”¢ Z-Score Norm        â”‚
â”‚  Adaptive learning       â”‚  Cosine warmup &       â”‚  Welford's online       â”‚
â”‚  with momentum           â”‚  decay strategy        â”‚  statistics             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

</div>

### Feature Breakdown

| Feature                            | Description                           | Benefit                             |
| ---------------------------------- | ------------------------------------- | ----------------------------------- |
| ğŸ”„ **Incremental Online Learning** | Process samples one at a time         | Memory efficient, real-time updates |
| ğŸ§® **Convolutional Architecture**  | 1D convolutions extract patterns      | Captures local dependencies in data |
| ğŸ“Š **Multivariate Support**        | Multiple inputs â†’ Multiple outputs    | Complex relationship modeling       |
| âš¡ **Adam Optimizer**              | Adaptive learning rates per parameter | Faster convergence, stable training |
| ğŸ“‰ **Cosine Warmup Schedule**      | Gradual LR increase then decay        | Prevents early divergence           |
| ğŸ”¢ **Welford's Algorithm**         | Online mean/variance computation      | Numerically stable normalization    |
| ğŸ›¡ï¸ **Outlier Detection**           | Z-score based anomaly detection       | Robust to noisy data                |
| ğŸ“ˆ **ADWIN Drift Detection**       | Adaptive windowing algorithm          | Handles concept drift               |
| ğŸ¯ **Uncertainty Estimation**      | Confidence intervals on predictions   | Quantified prediction reliability   |
| ğŸ”’ **L2 Regularization**           | Weight decay penalty                  | Prevents overfitting                |

---

## ğŸš€ Quick Start

### Installation

```typescript
// JSR
import { ConvolutionalRegression } from "jsr:@hviana/multivariate-convolutional-regression";
```

### Basic Usage

```typescript
import { ConvolutionalRegression } from "jsr:@hviana/multivariate-convolutional-regression";

// 1ï¸âƒ£ Create model instance
const model = new ConvolutionalRegression();

// 2ï¸âƒ£ Train with streaming data
const result = model.fitOnline({
  xCoordinates: [[1, 2, 3], [4, 5, 6], [7, 8, 9]],
  yCoordinates: [[10, 20], [30, 40], [50, 60]],
});

console.log(`ğŸ“‰ Loss: ${result.loss.toFixed(4)}`);
console.log(`âœ… Converged: ${result.converged}`);

// 3ï¸âƒ£ Make predictions
const predictions = model.predict(5);

for (const pred of predictions.predictions) {
  console.log(
    `ğŸ¯ Predicted: [${pred.predicted.map((v) => v.toFixed(2)).join(", ")}]`,
  );
  console.log(
    `ğŸ“Š 95% CI: [${pred.lowerBound.map((v) => v.toFixed(2)).join(", ")}] - [${
      pred.upperBound.map((v) => v.toFixed(2)).join(", ")
    }]`,
  );
}
```

---

## ğŸ—ï¸ Architecture

### Neural Network Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         NETWORK ARCHITECTURE                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    INPUT                CONVOLUTIONAL LAYERS                    OUTPUT
  (Features)           (Pattern Extraction)                   (Predictions)

 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚         â”‚      â”‚   Conv1D + ReLU (Ã—N)    â”‚      â”‚                         â”‚
 â”‚  xâ‚     â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”     â”‚      â”‚                         â”‚
 â”‚  xâ‚‚     â”‚â”€â”€â”€â”€â”€â”€â”‚  â”‚Conv â”‚â”€â”€â”€â”€â”‚ReLU â”‚     â”‚      â”‚      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
 â”‚  xâ‚ƒ     â”‚      â”‚  â”‚ 1D  â”‚    â”‚     â”‚     â”‚â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”‚  Dense  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â–º Å·â‚, Å·â‚‚, ...
 â”‚  ...    â”‚      â”‚  â””â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”˜     â”‚      â”‚      â”‚  Layer  â”‚        â”‚
 â”‚  xâ‚™     â”‚      â”‚       Ã—hiddenLayers     â”‚      â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
 â”‚         â”‚      â”‚                         â”‚      â”‚                         â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                      â”‚                               â”‚
      â–¼                      â–¼                               â–¼
  Z-Score              He Init +                      Linear Output
  Normalize            Same Padding                   + Denormalize
```

### Data Flow Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            TRAINING PIPELINE                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Raw    â”‚    â”‚   Welford    â”‚    â”‚   Forward    â”‚    â”‚    Loss      â”‚
  â”‚  Data   â”‚â”€â”€â”€â–¶â”‚   Z-Score    â”‚â”€â”€â”€â–¶â”‚    Pass      â”‚â”€â”€â”€â–¶â”‚  Compute     â”‚
  â”‚ (x, y)  â”‚    â”‚   Normalize  â”‚    â”‚              â”‚    â”‚   (MSE)      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                                â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚   Outlier    â”‚    â”‚   Backward   â”‚    â”‚    Adam      â”‚    â”‚    ADWIN     â”‚
  â”‚   Check &    â”‚â”€â”€â”€â–¶â”‚    Pass      â”‚â”€â”€â”€â–¶â”‚   Update     â”‚â”€â”€â”€â–¶â”‚    Drift     â”‚
  â”‚  Downweight  â”‚    â”‚  (Gradients) â”‚    â”‚  (Weights)   â”‚    â”‚   Detection  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Mathematical Foundations

#### Conv1D Operation

```
y[c, i] = Î£â‚– Î£â±¼ W[c, k, j] Â· x[k, i + j - pad] + b[c]
```

#### ReLU Activation

```
f(x) = max(0, x)
```

#### Adam Optimizer Update Rules

```
m = Î²â‚Â·m + (1-Î²â‚)Â·g           # First moment estimate
v = Î²â‚‚Â·v + (1-Î²â‚‚)Â·gÂ²          # Second moment estimate
mÌ‚ = m / (1-Î²â‚áµ—)               # Bias-corrected first moment
vÌ‚ = v / (1-Î²â‚‚áµ—)               # Bias-corrected second moment
W = W - Î·Â·mÌ‚ / (âˆšvÌ‚ + Îµ)        # Parameter update
```

#### Learning Rate Schedule

```
Warmup (t â‰¤ warmupSteps):
    lr = baseLR Ã— (t / warmupSteps)

Cosine Decay (t > warmupSteps):
    progress = (t - warmupSteps) / (totalSteps - warmupSteps)
    lr = baseLR Ã— 0.5 Ã— (1 + cos(Ï€ Ã— progress))
```

---

## âš™ï¸ Configuration Parameters

### Complete Configuration Interface

```typescript
interface ConvolutionalRegressionConfig {
  hiddenLayers?: number; // Default: 2
  convolutionsPerLayer?: number; // Default: 32
  kernelSize?: number; // Default: 3
  learningRate?: number; // Default: 0.001
  warmupSteps?: number; // Default: 100
  totalSteps?: number; // Default: 10000
  beta1?: number; // Default: 0.9
  beta2?: number; // Default: 0.999
  epsilon?: number; // Default: 1e-8
  regularizationStrength?: number; // Default: 1e-4
  convergenceThreshold?: number; // Default: 1e-6
  outlierThreshold?: number; // Default: 3.0
  adwinDelta?: number; // Default: 0.002
}
```

---

### ğŸ”· Network Architecture Parameters

#### `hiddenLayers`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>2</td></tr>
<tr><td><b>Range</b></td><td>1 - 10</td></tr>
</table>

**Description:** Number of convolutional hidden layers in the network.

```
hiddenLayers = 1:     Input â†’ [Conv+ReLU] â†’ Dense â†’ Output
hiddenLayers = 2:     Input â†’ [Conv+ReLU] â†’ [Conv+ReLU] â†’ Dense â†’ Output
hiddenLayers = 3:     Input â†’ [Conv+ReLU]Ã—3 â†’ Dense â†’ Output
```

**ğŸ¯ Optimization Guide:**

| Scenario                    | Recommended Value | Reasoning                       |
| --------------------------- | ----------------- | ------------------------------- |
| Simple linear relationships | 1                 | Minimal complexity needed       |
| Standard regression tasks   | 2                 | Good balance of capacity/speed  |
| Complex nonlinear patterns  | 3-4               | More representation power       |
| Very high-dimensional data  | 4-6               | Hierarchical feature extraction |

```typescript
// Simple data pattern
const simpleModel = new ConvolutionalRegression({ hiddenLayers: 1 });

// Complex multi-scale patterns
const complexModel = new ConvolutionalRegression({ hiddenLayers: 4 });
```

---

#### `convolutionsPerLayer`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>32</td></tr>
<tr><td><b>Range</b></td><td>8 - 256</td></tr>
</table>

**Description:** Number of output channels (filters) per convolutional layer.

```
                     convolutionsPerLayer = 16
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    1 channel  â”€â”€â”€â”€â–¶â”‚  16 learnable filters  â”‚â”€â”€â”€â”€â–¶  16 channels
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                     convolutionsPerLayer = 64
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    1 channel  â”€â”€â”€â”€â–¶â”‚  64 learnable filters  â”‚â”€â”€â”€â”€â–¶  64 channels
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ¯ Optimization Guide:**

| Data Complexity            | Recommended Value | Total Parameters Impact |
| -------------------------- | ----------------- | ----------------------- |
| Low (< 10 features)        | 8-16              | ~500-2,000              |
| Medium (10-50 features)    | 32                | ~5,000-20,000           |
| High (50-200 features)     | 64-128            | ~50,000-200,000         |
| Very High (> 200 features) | 128-256           | ~200,000+               |

```typescript
// Lightweight model for simple patterns
const lightModel = new ConvolutionalRegression({
  convolutionsPerLayer: 16,
});

// Heavy model for complex patterns
const heavyModel = new ConvolutionalRegression({
  convolutionsPerLayer: 128,
});
```

---

#### `kernelSize`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>3</td></tr>
<tr><td><b>Range</b></td><td>1 - 11 (odd numbers)</td></tr>
</table>

**Description:** Size of the convolutional kernel (receptive field).

```
kernelSize = 3:                kernelSize = 5:                kernelSize = 7:
â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”                  â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”          â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”
â”‚ wâ‚â”‚ wâ‚‚â”‚ wâ‚ƒâ”‚                  â”‚wâ‚ â”‚wâ‚‚ â”‚wâ‚ƒ â”‚wâ‚„ â”‚wâ‚… â”‚          â”‚wâ‚ â”‚wâ‚‚ â”‚wâ‚ƒ â”‚wâ‚„ â”‚wâ‚… â”‚wâ‚† â”‚wâ‚‡ â”‚
â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜                  â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜          â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜
Local patterns                 Medium-range patterns          Long-range patterns
```

**ğŸ¯ Optimization Guide:**

| Pattern Type            | Recommended Value | Use Case                      |
| ----------------------- | ----------------- | ----------------------------- |
| Very local dependencies | 1                 | Point-wise transformations    |
| Local patterns          | 3                 | Most regression tasks         |
| Medium-range patterns   | 5                 | Time series with short trends |
| Long-range patterns     | 7-11              | Seasonal or cyclic data       |

```typescript
// Local feature extraction
const localModel = new ConvolutionalRegression({ kernelSize: 3 });

// Capture longer-range dependencies
const wideModel = new ConvolutionalRegression({ kernelSize: 7 });
```

---

### ğŸ”· Optimizer Parameters

#### `learningRate`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>0.001</td></tr>
<tr><td><b>Range</b></td><td>1e-5 - 0.1</td></tr>
</table>

**Description:** Base learning rate for the Adam optimizer.

```
Learning Rate Effect:
                        
     High LR (0.01)     â”‚    Medium LR (0.001)   â”‚    Low LR (0.0001)
     âš¡ Fast but risky   â”‚    âœ… Balanced          â”‚    ğŸ¢ Slow but stable
                        â”‚                        â”‚
         â•±â•²             â”‚         â•²              â”‚              
        â•±  â•²            â”‚          â•²             â”‚           â•²
       â•±    â•²           â”‚           â•²            â”‚            â•²
      â•±      â•²â”€â”€Loss    â”‚            â•²â”€â”€â”€Loss    â”‚             â•²â”€â”€â”€Loss
     â•±        â•²         â”‚             â•²          â”‚              â•²
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     May overshoot      â”‚    Converges well      â”‚    Very slow progress
```

**ğŸ¯ Optimization Guide:**

| Scenario            | Recommended Value | Notes                         |
| ------------------- | ----------------- | ----------------------------- |
| Initial experiments | 0.001             | Start here for most cases     |
| Large datasets      | 0.0001 - 0.001    | More samples = lower LR       |
| Small datasets      | 0.001 - 0.01      | Fewer samples = can be higher |
| Fine-tuning         | 0.0001 - 0.0005   | After initial training        |
| Unstable training   | 0.0001            | If loss is oscillating        |

```typescript
// Standard training
const standardModel = new ConvolutionalRegression({ learningRate: 0.001 });

// Conservative training for sensitive data
const conservativeModel = new ConvolutionalRegression({ learningRate: 0.0001 });

// Aggressive training for quick experiments
const aggressiveModel = new ConvolutionalRegression({ learningRate: 0.005 });
```

---

#### `warmupSteps`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>100</td></tr>
<tr><td><b>Range</b></td><td>0 - 1000</td></tr>
</table>

**Description:** Number of steps for linear learning rate warmup.

```
Learning Rate Schedule with Warmup:

     lr
      â”‚
  max â”‚                    â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
      â”‚                 â•±                     â•²
      â”‚              â•±                          â•²
      â”‚           â•±                               â•²
      â”‚        â•±                                    â•²
      â”‚     â•±                                         â•²
  0   â”‚__â•±____________________________________________â•²___
      â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ steps
              â”‚
         warmupSteps
```

**ğŸ¯ Optimization Guide:**

| Dataset Size       | Recommended Value | Reasoning                     |
| ------------------ | ----------------- | ----------------------------- |
| < 100 samples      | 10-20             | Quick warmup, limited data    |
| 100-1000 samples   | 50-100            | Standard warmup               |
| 1000-10000 samples | 100-200           | Moderate warmup               |
| > 10000 samples    | 200-500           | Extended warmup for stability |

```typescript
// Quick warmup for small datasets
const quickWarmup = new ConvolutionalRegression({ warmupSteps: 20 });

// Extended warmup for large datasets
const extendedWarmup = new ConvolutionalRegression({ warmupSteps: 300 });
```

---

#### `totalSteps`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>10000</td></tr>
<tr><td><b>Range</b></td><td>1000 - 1000000</td></tr>
</table>

**Description:** Total training steps for cosine decay schedule.

```
Effect of totalSteps on Learning Rate Decay:

totalSteps = 5000          totalSteps = 10000         totalSteps = 50000
   (Fast decay)               (Standard)               (Slow decay)

lrâ”‚  â•²                     lrâ”‚     â•²                  lrâ”‚          â•²
  â”‚   â•²                      â”‚      â•²                   â”‚           â•²
  â”‚    â•²                     â”‚       â•²                  â”‚            â•²
  â”‚     â•²                    â”‚        â•²                 â”‚             â•²
  â”‚      â•²                   â”‚         â•²                â”‚              â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ steps        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ steps      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ steps
```

**ğŸ¯ Optimization Guide:**

| Training Duration   | Recommended Value | Use Case                 |
| ------------------- | ----------------- | ------------------------ |
| Short experiments   | 1000-5000         | Quick prototyping        |
| Standard training   | 10000             | Most applications        |
| Long training       | 50000-100000      | Complex models           |
| Continuous learning | 100000+           | Streaming data scenarios |

```typescript
// Quick experiment
const quickModel = new ConvolutionalRegression({ totalSteps: 2000 });

// Long training for complex patterns
const longModel = new ConvolutionalRegression({ totalSteps: 50000 });
```

---

#### `beta1` & `beta2`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Defaults</b></td><td>Î²â‚ = 0.9, Î²â‚‚ = 0.999</td></tr>
<tr><td><b>Range</b></td><td>0.0 - 0.9999</td></tr>
</table>

**Description:** Exponential decay rates for Adam's moment estimates.

```
Î²â‚ (First Moment - Momentum):
    Controls how much past gradients influence current direction
    Higher Î²â‚ â†’ More momentum â†’ Smoother updates

Î²â‚‚ (Second Moment - RMSprop):
    Controls adaptive learning rate per parameter
    Higher Î²â‚‚ â†’ More stable â†’ Slower adaptation
```

**ğŸ¯ Optimization Guide:**

| Scenario           | Î²â‚   | Î²â‚‚     | Notes                       |
| ------------------ | ---- | ------ | --------------------------- |
| Standard (default) | 0.9  | 0.999  | Works for most cases        |
| Noisy gradients    | 0.9  | 0.9999 | More gradient smoothing     |
| Sparse features    | 0.95 | 0.999  | Higher momentum             |
| Fast adaptation    | 0.8  | 0.99   | Quicker response to changes |

```typescript
// Standard Adam
const standardAdam = new ConvolutionalRegression({
  beta1: 0.9,
  beta2: 0.999,
});

// More aggressive adaptation
const aggressiveAdam = new ConvolutionalRegression({
  beta1: 0.8,
  beta2: 0.99,
});
```

---

#### `epsilon`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>1e-8</td></tr>
<tr><td><b>Range</b></td><td>1e-10 - 1e-4</td></tr>
</table>

**Description:** Numerical stability constant to prevent division by zero.

```
Adam Update: W -= Î·Â·mÌ‚ / (âˆšvÌ‚ + Îµ)
                              â†‘
                     Prevents division by zero
                     when vÌ‚ is very small
```

**ğŸ¯ Optimization Guide:**

| Scenario             | Recommended Value | Notes                 |
| -------------------- | ----------------- | --------------------- |
| Standard             | 1e-8              | Default, works well   |
| Mixed precision      | 1e-4              | Prevents underflow    |
| Very small gradients | 1e-10             | More precision needed |

---

### ğŸ”· Regularization Parameters

#### `regularizationStrength`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>1e-4</td></tr>
<tr><td><b>Range</b></td><td>0 - 0.1</td></tr>
</table>

**Description:** L2 regularization (weight decay) strength.

```
Loss = MSE + (Î»/2)Â·â€–Wâ€–Â²
               â†‘
       regularizationStrength

Effect on Weights:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Î» = 0 (No reg.)    â”‚  Î» = 1e-4 (Light)  â”‚  Î» = 1e-2 (Heavy) â”‚
â”‚  Weights can grow   â”‚  Gentle constraint â”‚  Strong shrinkage â”‚
â”‚  unbounded          â”‚  on weight size    â”‚  towards zero     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ¯ Optimization Guide:**

| Scenario             | Recommended Value | Effect                |
| -------------------- | ----------------- | --------------------- |
| No regularization    | 0                 | Might overfit         |
| Light regularization | 1e-5 - 1e-4       | Subtle weight control |
| Standard             | 1e-4              | Good balance          |
| Heavy regularization | 1e-3 - 1e-2       | Prevents overfitting  |
| Very heavy           | 0.1               | Underfitting risk     |

```typescript
// No regularization (for very clean data)
const noReg = new ConvolutionalRegression({ regularizationStrength: 0 });

// Strong regularization (for noisy data or small datasets)
const strongReg = new ConvolutionalRegression({ regularizationStrength: 0.01 });
```

---

### ğŸ”· Convergence & Detection Parameters

#### `convergenceThreshold`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>1e-6</td></tr>
<tr><td><b>Range</b></td><td>1e-10 - 1e-3</td></tr>
</table>

**Description:** Gradient L2 norm threshold for declaring convergence.

```
Convergence Check:
    â€–âˆ‡Lâ€–â‚‚ < convergenceThreshold  â†’  converged = true

Gradient Norm over Training:
    â”‚
    â”‚â•²
    â”‚ â•²
    â”‚  â•²
    â”‚   â•²__
    â”‚      â•²__
â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† convergenceThreshold
    â”‚            â•²____
    â”‚                 â•²_______
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶
                                  time
```

**ğŸ¯ Optimization Guide:**

| Precision Need | Recommended Value | Training Time           |
| -------------- | ----------------- | ----------------------- |
| Quick training | 1e-4              | Faster, less precise    |
| Standard       | 1e-6              | Balanced                |
| High precision | 1e-8              | Slower, more accurate   |
| Research-grade | 1e-10             | Very slow, very precise |

```typescript
// Quick convergence for prototyping
const quickConverge = new ConvolutionalRegression({
  convergenceThreshold: 1e-4,
});

// Strict convergence for production
const strictConverge = new ConvolutionalRegression({
  convergenceThreshold: 1e-8,
});
```

---

#### `outlierThreshold`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>3.0</td></tr>
<tr><td><b>Range</b></td><td>1.5 - 5.0</td></tr>
</table>

**Description:** Z-score threshold for outlier detection and downweighting.

```
Outlier Detection:
    z = |y - Å·| / Ïƒ
    
    if z > outlierThreshold:
        sample_weight = 0.1  (downweighted)
    else:
        sample_weight = 1.0  (normal)

Distribution of Z-scores:
                    
    â”‚        â•­â”€â”€â”€â”€â”€â•®        
    â”‚      â•±         â•²      
    â”‚    â•±             â•²    
    â”‚  â•±                 â•²  
    â”‚â•±                     â•²
â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   -3Ïƒ   -2Ïƒ   -1Ïƒ    0   +1Ïƒ   +2Ïƒ   +3Ïƒ
    â”‚                              â”‚     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Normal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
                                  Outlier Zone
```

**ğŸ¯ Optimization Guide:**

| Data Quality    | Recommended Value | Detection Rate             |
| --------------- | ----------------- | -------------------------- |
| Very clean data | 4.0-5.0           | Very few outliers detected |
| Standard data   | 3.0               | ~0.3% flagged as outliers  |
| Noisy data      | 2.5               | ~1.2% flagged              |
| Very noisy      | 2.0               | ~4.5% flagged              |

```typescript
// Sensitive to outliers (clean data expected)
const sensitiveModel = new ConvolutionalRegression({ outlierThreshold: 4.0 });

// Robust to outliers (noisy data)
const robustModel = new ConvolutionalRegression({ outlierThreshold: 2.5 });
```

---

#### `adwinDelta`

<table>
<tr><td><b>Type</b></td><td>number</td></tr>
<tr><td><b>Default</b></td><td>0.002</td></tr>
<tr><td><b>Range</b></td><td>0.0001 - 0.1</td></tr>
</table>

**Description:** ADWIN algorithm confidence parameter for drift detection.

```
ADWIN Drift Detection:

                    Window of recent losses
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Î¼â‚€ (old mean)   â”‚   Î¼â‚ (new mean)             â”‚
    â”‚     = 0.05       â”‚      = 0.15                 â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
                    Split point

    Drift detected if: |Î¼â‚€ - Î¼â‚| â‰¥ âˆš((1/2m)Â·ln(4n/Î´))
                                              â†‘
                                         adwinDelta
```

**ğŸ¯ Optimization Guide:**

| Sensitivity       | Recommended Value | Drift Detection       |
| ----------------- | ----------------- | --------------------- |
| Very sensitive    | 0.0001            | Detects small changes |
| Standard          | 0.002             | Balanced detection    |
| Conservative      | 0.01              | Only major drifts     |
| Very conservative | 0.1               | Very few false alarms |

```typescript
// Sensitive drift detection
const sensitiveDrift = new ConvolutionalRegression({ adwinDelta: 0.0005 });

// Conservative drift detection
const conservativeDrift = new ConvolutionalRegression({ adwinDelta: 0.01 });
```

---

## ğŸ“– API Reference

### Constructor

```typescript
new ConvolutionalRegression(config?: ConvolutionalRegressionConfig)
```

### Methods Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            PUBLIC METHODS                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚  ğŸ“Š Training                                                              â”‚
â”‚  â”œâ”€â”€ fitOnline(data)           Train incrementally on new samples        â”‚
â”‚  â”‚                                                                        â”‚
â”‚  ğŸ”® Prediction                                                            â”‚
â”‚  â”œâ”€â”€ predict(steps)            Generate future predictions               â”‚
â”‚  â”‚                                                                        â”‚
â”‚  ğŸ“ˆ Inspection                                                            â”‚
â”‚  â”œâ”€â”€ getModelSummary()         Get architecture & training summary       â”‚
â”‚  â”œâ”€â”€ getWeights()              Get all weights & optimizer state         â”‚
â”‚  â”œâ”€â”€ getNormalizationStats()   Get mean/std statistics                   â”‚
â”‚  â”‚                                                                        â”‚
â”‚  ğŸ’¾ Persistence                                                           â”‚
â”‚  â”œâ”€â”€ save()                    Serialize model to JSON string            â”‚
â”‚  â”œâ”€â”€ load(json)                Restore model from JSON string            â”‚
â”‚  â”‚                                                                        â”‚
â”‚  ğŸ”„ Management                                                            â”‚
â”‚  â””â”€â”€ reset()                   Clear all state, reinitialize            â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### `fitOnline(data)`

Train the model incrementally with new samples.

**Signature:**

```typescript
fitOnline(data: { 
    xCoordinates: number[][], 
    yCoordinates: number[][] 
}): FitResult
```

**Parameters:**

| Parameter      | Type         | Description             |
| -------------- | ------------ | ----------------------- |
| `xCoordinates` | `number[][]` | Array of input vectors  |
| `yCoordinates` | `number[][]` | Array of target vectors |

**Returns: `FitResult`**

```typescript
interface FitResult {
  loss: number; // MSE loss for this batch
  gradientNorm: number; // L2 norm of gradients
  effectiveLearningRate: number; // Current LR after schedule
  isOutlier: boolean; // Any outlier detected?
  converged: boolean; // Gradient < threshold?
  sampleIndex: number; // Total samples processed
  driftDetected: boolean; // ADWIN detected drift?
}
```

**Example:**

```typescript
const model = new ConvolutionalRegression();

// Single batch training
const result = model.fitOnline({
  xCoordinates: [[1, 2, 3], [4, 5, 6]],
  yCoordinates: [[10], [20]],
});

console.log(`Loss: ${result.loss}`);
console.log(`Converged: ${result.converged}`);

// Streaming training
for await (const batch of dataStream) {
  const result = model.fitOnline(batch);

  if (result.driftDetected) {
    console.log("âš ï¸ Concept drift detected!");
  }

  if (result.converged) {
    console.log("âœ… Model has converged");
    break;
  }
}
```

---

### `predict(futureSteps)`

Generate predictions with uncertainty estimates.

**Signature:**

```typescript
predict(futureSteps: number): PredictionResult
```

**Returns: `PredictionResult`**

```typescript
interface PredictionResult {
  predictions: SinglePrediction[]; // Array of predictions
  accuracy: number; // Model accuracy metric
  sampleCount: number; // Training samples seen
  isModelReady: boolean; // Enough training data?
}

interface SinglePrediction {
  predicted: number[]; // Point predictions
  lowerBound: number[]; // 95% CI lower bound
  upperBound: number[]; // 95% CI upper bound
  standardError: number[]; // Standard error per output
}
```

**Example:**

```typescript
const result = model.predict(5);

if (!result.isModelReady) {
  console.log("âš ï¸ Model needs more training data");
}

console.log(`ğŸ“Š Model Accuracy: ${(result.accuracy * 100).toFixed(2)}%`);

for (let i = 0; i < result.predictions.length; i++) {
  const pred = result.predictions[i];
  console.log(`Step ${i + 1}:`);
  console.log(`  Predicted: ${pred.predicted}`);
  console.log(`  95% CI: [${pred.lowerBound}, ${pred.upperBound}]`);
  console.log(`  Std Error: ${pred.standardError}`);
}
```

---

### `getModelSummary()`

Get comprehensive model information.

**Returns: `ModelSummary`**

```typescript
interface ModelSummary {
  isInitialized: boolean; // Has model been trained?
  inputDimension: number; // Number of input features
  outputDimension: number; // Number of outputs
  hiddenLayers: number; // Conv layer count
  convolutionsPerLayer: number; // Channels per layer
  kernelSize: number; // Convolution kernel size
  totalParameters: number; // Trainable parameter count
  sampleCount: number; // Training samples seen
  accuracy: number; // Current accuracy
  converged: boolean; // Has converged?
  effectiveLearningRate: number; // Current learning rate
  driftCount: number; // Number of drifts detected
}
```

**Example:**

```typescript
const summary = model.getModelSummary();

console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           MODEL SUMMARY                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Architecture                             â•‘
â•‘   Input Dimension:    ${
  summary.inputDimension.toString().padStart(6)
}         â•‘
â•‘   Output Dimension:   ${
  summary.outputDimension.toString().padStart(6)
}         â•‘
â•‘   Hidden Layers:      ${summary.hiddenLayers.toString().padStart(6)}         â•‘
â•‘   Convolutions/Layer: ${
  summary.convolutionsPerLayer.toString().padStart(6)
}         â•‘
â•‘   Kernel Size:        ${summary.kernelSize.toString().padStart(6)}         â•‘
â•‘   Total Parameters:   ${
  summary.totalParameters.toString().padStart(6)
}         â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Training Status                          â•‘
â•‘   Samples Processed:  ${summary.sampleCount.toString().padStart(6)}         â•‘
â•‘   Accuracy:           ${
  (summary.accuracy * 100).toFixed(2).padStart(5)
}%        â•‘
â•‘   Converged:          ${summary.converged.toString().padStart(6)}         â•‘
â•‘   Drift Events:       ${summary.driftCount.toString().padStart(6)}         â•‘
â•‘   Current LR:         ${
  summary.effectiveLearningRate.toExponential(2).padStart(10)
}   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
`);
```

---

### `getWeights()`

Retrieve all model weights and optimizer state.

**Returns: `WeightInfo`**

```typescript
interface WeightInfo {
  kernels: number[][][]; // Layer kernels
  biases: number[][][]; // Layer biases
  firstMoment: number[][][]; // Adam m values
  secondMoment: number[][][]; // Adam v values
  updateCount: number; // Number of Adam updates
}
```

---

### `getNormalizationStats()`

Get input/output normalization statistics.

**Returns: `NormalizationStats`**

```typescript
interface NormalizationStats {
  inputMean: number[]; // Mean of input features
  inputStd: number[]; // Std of input features
  outputMean: number[]; // Mean of outputs
  outputStd: number[]; // Std of outputs
  count: number; // Number of samples
}
```

---

### `save()` & `load()`

Serialize and restore complete model state.

**Example:**

```typescript
// Save model
const savedState = model.save();
localStorage.setItem("myModel", savedState);

// Later: restore model
const newModel = new ConvolutionalRegression();
const savedState = localStorage.getItem("myModel");
if (savedState) {
  newModel.load(savedState);
  console.log("âœ… Model restored successfully");
}
```

---

### `reset()`

Clear all model state and return to uninitialized.

```typescript
model.reset();
// Model is now fresh, ready for new training
```

---

## ğŸ¯ Optimization Guide

### Scenario-Based Configuration

#### ğŸ“ˆ Time Series Forecasting

```typescript
const timeSeriesModel = new ConvolutionalRegression({
  hiddenLayers: 3,
  convolutionsPerLayer: 64,
  kernelSize: 5, // Capture temporal patterns
  learningRate: 0.0005,
  warmupSteps: 200,
  totalSteps: 50000,
  regularizationStrength: 1e-4,
  adwinDelta: 0.001, // Sensitive to distribution shifts
});
```

#### ğŸ­ Industrial Sensor Data (Noisy)

```typescript
const industrialModel = new ConvolutionalRegression({
  hiddenLayers: 2,
  convolutionsPerLayer: 32,
  kernelSize: 3,
  learningRate: 0.001,
  outlierThreshold: 2.5, // More robust to outliers
  regularizationStrength: 1e-3, // Stronger regularization
  adwinDelta: 0.002,
});
```

#### ğŸš€ Real-time Streaming Data

```typescript
const streamingModel = new ConvolutionalRegression({
  hiddenLayers: 1, // Lightweight
  convolutionsPerLayer: 16, // Fast inference
  kernelSize: 3,
  learningRate: 0.002, // Faster adaptation
  warmupSteps: 50, // Quick warmup
  totalSteps: 100000, // Long-running
  adwinDelta: 0.001, // Quick drift detection
});
```

#### ğŸ”¬ High-Precision Scientific Data

```typescript
const scientificModel = new ConvolutionalRegression({
  hiddenLayers: 4,
  convolutionsPerLayer: 128,
  kernelSize: 5,
  learningRate: 0.0001, // Careful learning
  warmupSteps: 500,
  totalSteps: 100000,
  convergenceThreshold: 1e-8, // Strict convergence
  regularizationStrength: 1e-5,
  outlierThreshold: 4.0, // Very clean data expected
});
```

#### ğŸ“± Edge Device (Resource Constrained)

```typescript
const edgeModel = new ConvolutionalRegression({
  hiddenLayers: 1,
  convolutionsPerLayer: 8, // Minimal footprint
  kernelSize: 3,
  learningRate: 0.001,
  warmupSteps: 20,
  totalSteps: 5000,
});
```

---

### Parameter Tuning Flowchart

```
                START
                  â”‚
                  â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚  Use default config   â”‚
      â”‚  & observe loss curve â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Loss decreasing?    â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚           â”‚
           YES          NO
            â”‚           â”‚
            â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Continue   â”‚  â”‚ Learning rate  â”‚
â”‚  training   â”‚  â”‚  too high?     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚      â”‚
                     YES     NO
                      â”‚      â”‚
                      â–¼      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚ Decrease â”‚  â”‚ Increase     â”‚
           â”‚ LR by 2x â”‚  â”‚ model size   â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚   Overfitting?        â”‚
      â”‚ (trainâ†“ but valâ†‘)     â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚           â”‚
           YES          NO
            â”‚           â”‚
            â–¼           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â†‘ L2 reg    â”‚  â”‚   Check for    â”‚
â”‚ â†“ model     â”‚  â”‚   convergence  â”‚
â”‚   capacity  â”‚  â”‚                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ Use Case Examples

### Example 1: Multi-Output Regression

```typescript
import { ConvolutionalRegression } from "jsr:@hviana/multivariate-convolutional-regression";

// Predict multiple outputs from multiple inputs
const model = new ConvolutionalRegression({
  hiddenLayers: 2,
  convolutionsPerLayer: 32,
});

// Training data: 3 inputs â†’ 2 outputs
const trainingData = {
  xCoordinates: [
    [1.0, 2.0, 3.0],
    [2.0, 3.0, 4.0],
    [3.0, 4.0, 5.0],
    [4.0, 5.0, 6.0],
  ],
  yCoordinates: [
    [6.0, 2.0], // sum, variance proxy
    [9.0, 2.0],
    [12.0, 2.0],
    [15.0, 2.0],
  ],
};

// Train
for (let epoch = 0; epoch < 100; epoch++) {
  const result = model.fitOnline(trainingData);

  if (epoch % 20 === 0) {
    console.log(`Epoch ${epoch}: Loss = ${result.loss.toFixed(6)}`);
  }
}

// Predict
const predictions = model.predict(3);
console.log("\nğŸ“Š Predictions:");
predictions.predictions.forEach((p, i) => {
  console.log(
    `  Step ${i + 1}: [${p.predicted.map((v) => v.toFixed(2)).join(", ")}]`,
  );
});
```

### Example 2: Continuous Learning with Drift Detection

```typescript
import { ConvolutionalRegression } from "jsr:@hviana/multivariate-convolutional-regression";

const model = new ConvolutionalRegression({
  adwinDelta: 0.001, // Sensitive drift detection
  learningRate: 0.001,
});

// Simulate streaming data with concept drift
async function processDataStream() {
  let phase = 1;

  for (let t = 0; t < 1000; t++) {
    // Generate data with drift at t=500
    const x = [Math.sin(t * 0.1), Math.cos(t * 0.1), t * 0.01];
    const y = phase === 1
      ? [x[0] + x[1]] // Phase 1: simple sum
      : [x[0] * x[1] + x[2]]; // Phase 2: different relationship

    if (t === 500) phase = 2; // Introduce drift

    const result = model.fitOnline({
      xCoordinates: [x],
      yCoordinates: [y],
    });

    if (result.driftDetected) {
      console.log(`ğŸ”„ Drift detected at t=${t}!`);
      console.log(`   Loss: ${result.loss.toFixed(4)}`);
    }

    // Periodic status
    if (t % 100 === 0) {
      const summary = model.getModelSummary();
      console.log(
        `t=${t}: Accuracy=${
          (summary.accuracy * 100).toFixed(1)
        }%, Drifts=${summary.driftCount}`,
      );
    }
  }
}

processDataStream();
```

### Example 3: Model Persistence

```typescript
import { ConvolutionalRegression } from "jsr:@hviana/multivariate-convolutional-regression";

// Training phase
async function train() {
  const model = new ConvolutionalRegression({
    hiddenLayers: 2,
    convolutionsPerLayer: 32,
  });

  // Train on data...
  for (let i = 0; i < 1000; i++) {
    model.fitOnline({
      xCoordinates: [[Math.random(), Math.random()]],
      yCoordinates: [[Math.random()]],
    });
  }

  // Save model
  const serialized = model.save();
  await Deno.writeTextFile("model.json", serialized);
  console.log("âœ… Model saved!");

  return model.getModelSummary();
}

// Inference phase
async function inference() {
  const model = new ConvolutionalRegression();

  // Load model
  const serialized = await Deno.readTextFile("model.json");
  model.load(serialized);
  console.log("âœ… Model loaded!");

  // Make predictions
  const result = model.predict(5);
  return result.predictions;
}

// Usage
const trainingSummary = await train();
console.log("Training completed:", trainingSummary);

const predictions = await inference();
console.log("Predictions:", predictions);
```

### Example 4: Uncertainty-Aware Predictions

```typescript
import { ConvolutionalRegression } from "jsr:@hviana/multivariate-convolutional-regression";

const model = new ConvolutionalRegression();

// Train model
// ... (training code)

// Get predictions with uncertainty
const result = model.predict(10);

console.log("\nğŸ“Š Predictions with Confidence Intervals:\n");
console.log("â”Œâ”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”");
console.log("â”‚ Step â”‚ Prediction â”‚      95% CI         â”‚ Std Error  â”‚");
console.log("â”œâ”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤");

result.predictions.forEach((pred, i) => {
  const prediction = pred.predicted[0].toFixed(3);
  const lower = pred.lowerBound[0].toFixed(3);
  const upper = pred.upperBound[0].toFixed(3);
  const stdErr = pred.standardError[0].toFixed(4);

  console.log(
    `â”‚ ${(i + 1).toString().padStart(4)} â”‚ ${
      prediction.padStart(10)
    } â”‚ [${lower}, ${upper}] â”‚ ${stdErr.padStart(10)} â”‚`,
  );
});

console.log("â””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜");

// Use uncertainty for decision making
const criticalPredictions = result.predictions.filter(
  (p) => p.standardError[0] > 0.5,
);

if (criticalPredictions.length > 0) {
  console.log("\nâš ï¸ Warning: Some predictions have high uncertainty!");
}
```

---

## ğŸ”§ Advanced Topics

### Internal Algorithms

#### Welford's Online Statistics

```typescript
// Numerically stable one-pass mean/variance computation
update(x):
    n += 1
    Î´ = x - Î¼
    Î¼ += Î´ / n
    Mâ‚‚ += Î´ Ã— (x - Î¼)
    ÏƒÂ² = Mâ‚‚ / (n - 1)
```

#### ADWIN Drift Detection

```typescript
// Adaptive windowing for distribution change detection
Drift condition: |Î¼_old - Î¼_new| â‰¥ âˆš((1/2m) Ã— ln(4n/Î´))
    where m = harmonic mean of window sizes
```

#### He Weight Initialization

```typescript
// Optimal initialization for ReLU networks
W ~ N(0, âˆš(2/fan_in))
```

### Memory Optimization

The library uses `Float64Array` for all numerical computations and preallocates
buffers to minimize garbage collection pressure:

```typescript
// Internal buffer management
- Activation buffers: Preallocated per layer
- Gradient buffers: Reused across backward passes
- Normalization buffers: Persistent across calls
```

### Performance Tips

1. **Batch Similar Samples**: Group similar samples in `fitOnline()` calls
2. **Monitor Convergence**: Stop training when `converged === true`
3. **Use Appropriate Model Size**: Start small, increase if underfitting
4. **Enable Drift Detection**: Use ADWIN for streaming scenarios

---

## ğŸ“Š Comparison with Traditional Methods

| Feature           | This Library | Traditional Polynomial | Deep Learning |
| ----------------- | ------------ | ---------------------- | ------------- |
| Online Learning   | âœ… Native    | âŒ Batch only          | âš ï¸ Complex    |
| Memory Efficiency | âœ… Constant  | âŒ O(n)                | âš ï¸ Variable   |
| Concept Drift     | âœ… ADWIN     | âŒ None                | âŒ Manual     |
| Uncertainty       | âœ… Built-in  | âš ï¸ Limited             | âŒ Separate   |
| Dependencies      | âœ… Zero      | âœ… Zero                | âŒ Many       |
| Setup Complexity  | âœ… Simple    | âœ… Simple              | âŒ Complex    |

---

## ğŸ“„ License

**MIT License** Â© 2025 Henrique Emanoel Viana

---

## ğŸ‘¤ Author

<div align="center">

**Henrique Emanoel Viana**

[ğŸ™ GitHub](https://github.com/hviana) â€¢ [ğŸ“¦ JSR](https://jsr.io/@hviana)

</div>

---

<div align="center">

Made with â¤ï¸ for the community

**â­ Star this repo if you find it useful!**

</div>
